INFO 05-10 03:08:00 llm_engine.py:75] Initializing an LLM engine (v0.4.0) with config: model='/users/zyh/models/llama2-7B/', tokenizer='/users/zyh/models/llama2-7B/', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)
INFO 05-10 03:08:00 selector.py:45] Cannot use FlashAttention because the package is not found. Please install it for better performance.
INFO 05-10 03:08:00 selector.py:21] Using XFormers backend.
INFO 05-10 03:08:19 model_runner.py:104] Loading model weights took 12.5523 GB
INFO 05-10 03:08:20 gpu_executor.py:94] # GPU blocks: 2848, # CPU blocks: 512
INFO 05-10 03:08:24 model_runner.py:791] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 05-10 03:08:24 model_runner.py:795] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 05-10 03:08:31 model_runner.py:867] Graph capturing finished in 7 secs.
warming up...
INFO 05-10 03:08:37 metrics.py:218] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%
INFO 05-10 03:08:42 metrics.py:218] Avg prompt throughput: 3550.2 tokens/s, Avg generation throughput: 43.3 tokens/s, Running: 213 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 43.0%, CPU KV cache usage: 0.0%
INFO 05-10 03:08:47 metrics.py:218] Avg prompt throughput: 408.8 tokens/s, Avg generation throughput: 2675.9 tokens/s, Running: 256 reqs, Swapped: 0 reqs, Pending: 48 reqs, GPU KV cache usage: 67.1%, CPU KV cache usage: 0.0%
INFO 05-10 03:08:52 metrics.py:218] Avg prompt throughput: 125.8 tokens/s, Avg generation throughput: 3240.2 tokens/s, Running: 256 reqs, Swapped: 0 reqs, Pending: 110 reqs, GPU KV cache usage: 99.2%, CPU KV cache usage: 0.0%
INFO 05-10 03:08:57 metrics.py:218] Avg prompt throughput: 4.6 tokens/s, Avg generation throughput: 2866.4 tokens/s, Running: 166 reqs, Swapped: 0 reqs, Pending: 252 reqs, GPU KV cache usage: 99.2%, CPU KV cache usage: 0.0%
INFO 05-10 03:09:02 metrics.py:218] Avg prompt throughput: 340.8 tokens/s, Avg generation throughput: 1927.7 tokens/s, Running: 56 reqs, Swapped: 0 reqs, Pending: 226 reqs, GPU KV cache usage: 27.2%, CPU KV cache usage: 0.0%
INFO 05-10 03:09:07 metrics.py:218] Avg prompt throughput: 2620.2 tokens/s, Avg generation throughput: 1980.0 tokens/s, Running: 256 reqs, Swapped: 0 reqs, Pending: 4 reqs, GPU KV cache usage: 73.9%, CPU KV cache usage: 0.0%
INFO 05-10 03:09:12 metrics.py:218] Avg prompt throughput: 57.9 tokens/s, Avg generation throughput: 3164.8 tokens/s, Running: 193 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 77.6%, CPU KV cache usage: 0.0%
INFO 05-10 03:09:17 metrics.py:218] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 2754.0 tokens/s, Running: 154 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 85.7%, CPU KV cache usage: 0.0%

num of requests processed: 500
avg time: 23.127233618736266
block_size =  16
